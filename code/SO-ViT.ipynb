{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb843432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\decomp\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from decomp_utils import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6c262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"~\"\n",
    "train_ls = [1, 3]\n",
    "test_ls = 2\n",
    "overlap = 5 #ms\n",
    "x_sEMG = 'EMGs'\n",
    "y_spikes = 'Spikes'\n",
    "mu = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d232506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_train(path, train_ls, overlap,mu)\n",
    "x = np.expand_dims(x, axis=3)   #2D Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da2b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_motor = 9\n",
    "y = y[select_motor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877ece78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x.shape[1:]\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c1d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameter\n",
    "\n",
    "input_shape = input_shape\n",
    "patch_size = 12\n",
    "no_patches = 50  # h * w / p^2\n",
    "dims = 100\n",
    "no_transformer_layers = 4\n",
    "no_heads = 2\n",
    "transformer_units = [dims * 2,dims]  # Size of the transformer layers\n",
    "mlp_head_units = [240, 10]   # [2048, 1024]\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3b2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_Patches(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, patch_size):\n",
    "        super(Generate_Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    \n",
    "    def call(self, images):\n",
    "        \n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(images=images,\n",
    "                           sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "                           strides=[1, self.patch_size, self.patch_size, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')\n",
    "        h_w_c = patches.shape[-1]\n",
    "        patch_reshape = tf.reshape(patches, [batch_size, -1, h_w_c]) # (batch_size, no_of_patches, h*w*c)\n",
    "        \n",
    "        return patch_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0c7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed_Position(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_patches, projection_dims):\n",
    "        \n",
    "        super(Embed_Position, self).__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.project = tf.keras.layers.Dense(units=projection_dims)\n",
    "        self.embed = tf.keras.layers.Embedding(input_dim=num_patches, output_dim=projection_dims)\n",
    "    \n",
    "    def call(self, patch):\n",
    "        \n",
    "        position = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encode = self.project(patch) + self.embed(position)\n",
    "        \n",
    "        return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b5e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_mixer(inputs, no_units, drop_out):\n",
    "    \n",
    "    for units in no_units:\n",
    "        x = tf.keras.layers.Dense(units=units, activation = tf.nn.gelu)(inputs)\n",
    "        x = tf.keras.layers.Dropout(drop_out)(x)\n",
    "        # x = tf.keras.layers.Dense(inputs.shape[-1], activation=tf.nn.gelu)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120d45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_trans_decomp():\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    #x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    returned_patches = Generate_Patches(patch_size)(inputs)\n",
    "    encoded_patches = Embed_Position(no_patches, dims)(returned_patches)\n",
    "    #encoded_patches = tf.keras.layers.BatchNormalization()(encoded_patches)\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(no_transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        input_two_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(num_heads=no_heads, key_dim=dims, dropout=0.1)(input_two_attention, input_two_attention)\n",
    "        # Skip connection 1.\n",
    "        input_two_attention_2 = tf.keras.layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        input_two_attention_3  = tf.keras.layers.LayerNormalization(epsilon=1e-6)(input_two_attention_2)\n",
    "        # MLP.\n",
    "        input_two_attention_3 = mlp_mixer(input_two_attention_3, no_units=transformer_units, drop_out=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = tf.keras.layers.Add()([input_two_attention_3, input_two_attention_3])\n",
    "        \n",
    "        \n",
    "# Create a [batch_size, projection_dim] tensor.\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = tf.keras.layers.Flatten()(representation)\n",
    "    representation = tf.keras.layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp_mixer(representation, no_units=mlp_head_units, drop_out=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = tf.keras.layers.Dense(1, activation='sigmoid')(features)\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits, name=\"sEMG-Decomposition\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df8ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sEMG-Decomposition\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 120, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " generate__patches (Generate_Pa  (None, None, 144)   0           ['input_1[0][0]']                \n",
      " tches)                                                                                           \n",
      "                                                                                                  \n",
      " embed__position (Embed_Positio  (None, 50, 100)     19500       ['generate__patches[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 50, 100)     200         ['embed__position[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 50, 100)     80700       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 50, 100)      0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'embed__position[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 50, 100)     200         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 100)      10100       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 100)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 50, 100)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 50, 100)     200         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 50, 100)     80700       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 50, 100)      0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 50, 100)     200         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50, 100)      10100       ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 50, 100)      0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 50, 100)      0           ['dropout_3[0][0]',              \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 50, 100)     200         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 50, 100)     80700       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 50, 100)      0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 50, 100)     200         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 50, 100)      10100       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 50, 100)      0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 50, 100)      0           ['dropout_5[0][0]',              \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 50, 100)     200         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 50, 100)     80700       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 50, 100)      0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 50, 100)     200         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 50, 100)      10100       ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_7 (Dropout)            (None, 50, 100)      0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 50, 100)      0           ['dropout_7[0][0]',              \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 50, 100)     200         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 5000)         0           ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 5000)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 10)           50010       ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 10)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 434,521\n",
      "Trainable params: 434,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = vit_trans_decomp()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcc0fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_callback = AccuracyCallback('accuracy')\n",
    "f1_callback = AccuracyCallback('f1_m')\n",
    "\n",
    "\n",
    "n_batch = 128\n",
    "n_epochs = 3\n",
    "ls =  'binary_crossentropy'\n",
    "mtr = ['mse', 'accuracy',  f1_m]\n",
    "opt = 'rmsprop'\n",
    "model.compile(optimizer=opt, loss=ls, metrics=mtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc683179",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "129/129 [==============================] - 15s 66ms/step - loss: 0.1783 - mse: 0.0358 - accuracy: 0.9738 - f1_m: 0.0100 - val_loss: 0.0775 - val_mse: 0.0121 - val_accuracy: 0.9879 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/3\n",
      "129/129 [==============================] - 7s 54ms/step - loss: 0.1538 - mse: 0.0296 - accuracy: 0.9844 - f1_m: 0.0022 - val_loss: 0.1020 - val_mse: 0.0121 - val_accuracy: 0.9879 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/3\n",
      "129/129 [==============================] - 7s 52ms/step - loss: 0.1423 - mse: 0.0274 - accuracy: 0.9854 - f1_m: 0.0000e+00 - val_loss: 0.0879 - val_mse: 0.0121 - val_accuracy: 0.9879 - val_f1_m: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x,y, validation_split=0.2, batch_size=n_batch, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ed720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Validation\n",
    "\n",
    "x_ts, y_ts = load_test(path, test_ls, overlap,mu)\n",
    "x_ts = np.expand_dims(x_ts, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e264fb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - 6s 17ms/step - loss: 0.2022 - mse: 0.0281 - accuracy: 0.9718 - f1_m: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20221994817256927, 0.028120050206780434, 0.9718213081359863, 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_ts, y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1d2daaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - 4s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_ts)\n",
    "y_pred = np.argmax(pred, axis=-1)\n",
    "confusion = confusion_matrix(y_ts[select_motor], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "105c701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[0, 0]\n",
    "TN = confusion[1, 1]\n",
    "FP = confusion[1, 0]\n",
    "FN = confusion[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daaba938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.9919489445262641\n"
     ]
    }
   ],
   "source": [
    "precision = TP / float(TP + FP)\n",
    "print(\"Precision is {}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e87410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall is 1.0\n"
     ]
    }
   ],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "print(\"recall is {}\".format(recall))\n",
    "\n",
    "#RECALL SAME AS SENSITIVITY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decomp",
   "language": "python",
   "name": "decomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
