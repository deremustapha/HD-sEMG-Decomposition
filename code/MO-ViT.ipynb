{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33060032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\decomp\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from decomp_utils import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc67613",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\AI-Workspace\\\\Decomposition\\\\ViT_Decompose\\\\data\\\\5_50_GM-\"\n",
    "train_ls = [1, 3]\n",
    "test_ls = 2\n",
    "overlap = 5\n",
    "x_sEMG = 'EMGs'\n",
    "y_spikes = 'Spikes'\n",
    "mu = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3100fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_train(path, train_ls, overlap,mu)\n",
    "x = np.expand_dims(x, axis=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f928783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x.shape[1:]\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a29751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameter\n",
    "\n",
    "input_shape = input_shape\n",
    "patch_size = 12\n",
    "no_patches = 50  # h * w / p^2\n",
    "dims = 100\n",
    "no_transformer_layers = 4\n",
    "no_heads = 2\n",
    "transformer_units = [dims * 2,dims]  # Size of the transformer layers\n",
    "mlp_head_units = [240, 10]   # [2048, 1024]\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869b67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_Patches(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, patch_size):\n",
    "        super(Generate_Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    \n",
    "    def call(self, images):\n",
    "        \n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(images=images,\n",
    "                           sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "                           strides=[1, self.patch_size, self.patch_size, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')\n",
    "        h_w_c = patches.shape[-1]\n",
    "        patch_reshape = tf.reshape(patches, [batch_size, -1, h_w_c]) # (batch_size, no_of_patches, h*w*c)\n",
    "        \n",
    "        return patch_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7363dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed_Position(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_patches, projection_dims):\n",
    "        \n",
    "        super(Embed_Position, self).__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.project = tf.keras.layers.Dense(units=projection_dims)\n",
    "        self.embed = tf.keras.layers.Embedding(input_dim=num_patches, output_dim=projection_dims)\n",
    "    \n",
    "    def call(self, patch):\n",
    "        \n",
    "        position = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encode = self.project(patch) + self.embed(position)\n",
    "        \n",
    "        return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea8b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_mixer(inputs, no_units, drop_out):\n",
    "    \n",
    "    for units in no_units:\n",
    "        x = tf.keras.layers.Dense(units=units, activation = tf.nn.gelu)(inputs)\n",
    "        x = tf.keras.layers.Dropout(drop_out)(x)\n",
    "        # x = tf.keras.layers.Dense(inputs.shape[-1], activation=tf.nn.gelu)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d21b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mo_vit_trans_decomp():\n",
    "    \n",
    "    outputs = []\n",
    "    no_of_nodes = len(mu)\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    #x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    returned_patches = Generate_Patches(patch_size)(inputs)\n",
    "    encoded_patches = Embed_Position(no_patches, dims)(returned_patches)\n",
    "    #encoded_patches = tf.keras.layers.BatchNormalization()(encoded_patches)\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(no_transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        input_two_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(num_heads=no_heads, key_dim=dims, dropout=0.1)(input_two_attention, input_two_attention)\n",
    "        # Skip connection 1.\n",
    "        input_two_attention_2 = tf.keras.layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        input_two_attention_3  = tf.keras.layers.LayerNormalization(epsilon=1e-6)(input_two_attention_2)\n",
    "        # MLP.\n",
    "        input_two_attention_3 = mlp_mixer(input_two_attention_3, no_units=transformer_units, drop_out=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = tf.keras.layers.Add()([input_two_attention_3, input_two_attention_3])\n",
    "        \n",
    "        \n",
    "# Create a [batch_size, projection_dim] tensor.\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = tf.keras.layers.Flatten()(representation)\n",
    "    representation = tf.keras.layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp_mixer(representation, no_units=mlp_head_units, drop_out=0.5)\n",
    "    # Classify outputs.\n",
    "    \n",
    "    for i in range(1, no_of_nodes+1):\n",
    "        output = tf.keras.layers.Dense(output_shape, activation='sigmoid', name='output_{}'.format(i))(features)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"sEMG-Decomposition\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbd65d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sEMG-Decomposition\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 120, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " generate__patches (Generate_Pa  (None, None, 144)   0           ['input_1[0][0]']                \n",
      " tches)                                                                                           \n",
      "                                                                                                  \n",
      " embed__position (Embed_Positio  (None, 50, 100)     19500       ['generate__patches[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 50, 100)     200         ['embed__position[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 50, 100)     80700       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 50, 100)      0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'embed__position[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 50, 100)     200         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 100)      10100       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 100)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 50, 100)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 50, 100)     200         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 50, 100)     80700       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 50, 100)      0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 50, 100)     200         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50, 100)      10100       ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 50, 100)      0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 50, 100)      0           ['dropout_3[0][0]',              \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 50, 100)     200         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 50, 100)     80700       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 50, 100)      0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 50, 100)     200         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 50, 100)      10100       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 50, 100)      0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 50, 100)      0           ['dropout_5[0][0]',              \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 50, 100)     200         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 50, 100)     80700       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 50, 100)      0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 50, 100)     200         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 50, 100)      10100       ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_7 (Dropout)            (None, 50, 100)      0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 50, 100)      0           ['dropout_7[0][0]',              \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 50, 100)     200         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 5000)         0           ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 5000)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 10)           50010       ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 10)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " output_1 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_2 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_3 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_4 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_5 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_6 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_7 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_8 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_9 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_10 (Dense)              (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 434,620\n",
      "Trainable params: 434,620\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = mo_vit_trans_decomp()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d57b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_callback = AccuracyCallback('accuracy')\n",
    "f1_callback = AccuracyCallback('f1_m')\n",
    "\n",
    "\n",
    "n_batch = 128\n",
    "n_epochs = 1\n",
    "ls =  'binary_crossentropy'\n",
    "mtr = ['mse', 'accuracy',  f1_m]\n",
    "opt = 'rmsprop'\n",
    "model.compile(optimizer=opt, loss=ls, metrics=mtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b31df261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 26s 116ms/step - loss: 4.2934 - output_1_loss: 1.2329 - output_2_loss: 0.3618 - output_3_loss: 0.3811 - output_4_loss: 0.3329 - output_5_loss: 0.2784 - output_6_loss: 0.5403 - output_7_loss: 0.3051 - output_8_loss: 0.3234 - output_9_loss: 0.2874 - output_10_loss: 0.2500 - output_1_mse: 0.4403 - output_1_accuracy: 0.3336 - output_1_f1_m: 0.0572 - output_2_mse: 0.1087 - output_2_accuracy: 0.9430 - output_2_f1_m: 0.0090 - output_3_mse: 0.1177 - output_3_accuracy: 0.9333 - output_3_f1_m: 0.0149 - output_4_mse: 0.0942 - output_4_accuracy: 0.9469 - output_4_f1_m: 0.0108 - output_5_mse: 0.0750 - output_5_accuracy: 0.9370 - output_5_f1_m: 0.0176 - output_6_mse: 0.1698 - output_6_accuracy: 0.8280 - output_6_f1_m: 0.0431 - output_7_mse: 0.0836 - output_7_accuracy: 0.9327 - output_7_f1_m: 0.0195 - output_8_mse: 0.0947 - output_8_accuracy: 0.9589 - output_8_f1_m: 0.0076 - output_9_mse: 0.0762 - output_9_accuracy: 0.9347 - output_9_f1_m: 0.0144 - output_10_mse: 0.0727 - output_10_accuracy: 0.9446 - output_10_f1_m: 0.0102 - val_loss: 1.9239 - val_output_1_loss: 0.7331 - val_output_2_loss: 0.1668 - val_output_3_loss: 0.1739 - val_output_4_loss: 0.1227 - val_output_5_loss: 0.0868 - val_output_6_loss: 0.2042 - val_output_7_loss: 0.1187 - val_output_8_loss: 0.1443 - val_output_9_loss: 0.0995 - val_output_10_loss: 0.0739 - val_output_1_mse: 0.2699 - val_output_1_accuracy: 0.0236 - val_output_1_f1_m: 0.0443 - val_output_2_mse: 0.0318 - val_output_2_accuracy: 0.9735 - val_output_2_f1_m: 0.0000e+00 - val_output_3_mse: 0.0318 - val_output_3_accuracy: 0.9774 - val_output_3_f1_m: 0.0000e+00 - val_output_4_mse: 0.0240 - val_output_4_accuracy: 0.9762 - val_output_4_f1_m: 0.0000e+00 - val_output_5_mse: 0.0169 - val_output_5_accuracy: 0.9828 - val_output_5_f1_m: 0.0000e+00 - val_output_6_mse: 0.0390 - val_output_6_accuracy: 0.9772 - val_output_6_f1_m: 0.0000e+00 - val_output_7_mse: 0.0243 - val_output_7_accuracy: 0.9752 - val_output_7_f1_m: 0.0000e+00 - val_output_8_mse: 0.0234 - val_output_8_accuracy: 0.9842 - val_output_8_f1_m: 0.0000e+00 - val_output_9_mse: 0.0200 - val_output_9_accuracy: 0.9796 - val_output_9_f1_m: 0.0000e+00 - val_output_10_mse: 0.0124 - val_output_10_accuracy: 0.9879 - val_output_10_f1_m: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x,y, validation_split=0.2, batch_size=n_batch, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "600cd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Validation\n",
    "\n",
    "x_ts, y_ts = load_test(path, test_ls, overlap,mu)\n",
    "x_ts = np.expand_dims(x_ts, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b7190a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - 10s 32ms/step - loss: 1.9633 - output_1_loss: 0.7328 - output_2_loss: 0.1741 - output_3_loss: 0.1823 - output_4_loss: 0.1294 - output_5_loss: 0.0963 - output_6_loss: 0.2108 - output_7_loss: 0.1316 - output_8_loss: 0.1452 - output_9_loss: 0.1004 - output_10_loss: 0.0603 - output_1_mse: 0.2698 - output_1_accuracy: 0.0282 - output_1_f1_m: 0.0540 - output_2_mse: 0.0344 - output_2_accuracy: 0.9703 - output_2_f1_m: 0.0000e+00 - output_3_mse: 0.0349 - output_3_accuracy: 0.9734 - output_3_f1_m: 0.0000e+00 - output_4_mse: 0.0261 - output_4_accuracy: 0.9739 - output_4_f1_m: 0.0000e+00 - output_5_mse: 0.0192 - output_5_accuracy: 0.9804 - output_5_f1_m: 0.0000e+00 - output_6_mse: 0.0417 - output_6_accuracy: 0.9733 - output_6_f1_m: 0.0000e+00 - output_7_mse: 0.0280 - output_7_accuracy: 0.9712 - output_7_f1_m: 0.0000e+00 - output_8_mse: 0.0237 - output_8_accuracy: 0.9840 - output_8_f1_m: 0.0000e+00 - output_9_mse: 0.0202 - output_9_accuracy: 0.9794 - output_9_f1_m: 0.0000e+00 - output_10_mse: 0.0086 - output_10_accuracy: 0.9919 - output_10_f1_m: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9632724523544312,\n",
       " 0.7328438758850098,\n",
       " 0.1740976721048355,\n",
       " 0.18227101862430573,\n",
       " 0.12944242358207703,\n",
       " 0.09632701426744461,\n",
       " 0.21081386506557465,\n",
       " 0.13159900903701782,\n",
       " 0.14520803093910217,\n",
       " 0.10036522895097733,\n",
       " 0.060304976999759674,\n",
       " 0.2698369324207306,\n",
       " 0.02817869372665882,\n",
       " 0.054045870900154114,\n",
       " 0.03442078456282616,\n",
       " 0.9702503681182861,\n",
       " 0.0,\n",
       " 0.0349278561770916,\n",
       " 0.9733922481536865,\n",
       " 0.0,\n",
       " 0.026127323508262634,\n",
       " 0.9738831520080566,\n",
       " 0.0,\n",
       " 0.019240327179431915,\n",
       " 0.9803632497787476,\n",
       " 0.0,\n",
       " 0.04170413687825203,\n",
       " 0.9732940793037415,\n",
       " 0.0,\n",
       " 0.028017982840538025,\n",
       " 0.9712321758270264,\n",
       " 0.0,\n",
       " 0.02366774156689644,\n",
       " 0.9839960932731628,\n",
       " 0.0,\n",
       " 0.02019151858985424,\n",
       " 0.9793814420700073,\n",
       " 0.0,\n",
       " 0.008581788279116154,\n",
       " 0.9919489622116089,\n",
       " 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_ts, y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "083136d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_motor = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "279bcf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - 4s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_ts)\n",
    "y_pred = np.argmax(pred[select_motor], axis=-1)\n",
    "confusion = confusion_matrix(y_ts[select_motor], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ebb7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[0, 0]\n",
    "TN = confusion[1, 1]\n",
    "FP = confusion[1, 0]\n",
    "FN = confusion[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "963e67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.9919489445262641\n"
     ]
    }
   ],
   "source": [
    "precision = TP / float(TP + FP)\n",
    "print(\"Precision is {}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7036f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall is 1.0\n"
     ]
    }
   ],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "print(\"recall is {}\".format(recall))\n",
    "\n",
    "#RECALL SAME AS SENSITIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34165b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: decompose\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: decompose\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('decompose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78dccf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c48db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decomp",
   "language": "python",
   "name": "decomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
