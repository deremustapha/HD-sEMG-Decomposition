{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33060032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\decomp\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from decomp_utils import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc67613",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"~\"\n",
    "train_ls = [1, 3]\n",
    "test_ls = 2\n",
    "overlap = 5\n",
    "x_sEMG = 'EMGs'\n",
    "y_spikes = 'Spikes'\n",
    "mu = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3100fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_train(path, train_ls, overlap,mu)\n",
    "x = np.expand_dims(x, axis=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f928783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x.shape[1:]\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a29751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameter\n",
    "\n",
    "input_shape = input_shape\n",
    "patch_size = 12\n",
    "no_patches = 50  # h * w / p^2\n",
    "dims = 100\n",
    "no_transformer_layers = 4\n",
    "no_heads = 2\n",
    "transformer_units = [dims * 2,dims]  # Size of the transformer layers\n",
    "mlp_head_units = [240, 10]   # [2048, 1024]\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869b67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_Patches(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, patch_size):\n",
    "        super(Generate_Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    \n",
    "    def call(self, images):\n",
    "        \n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(images=images,\n",
    "                           sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "                           strides=[1, self.patch_size, self.patch_size, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')\n",
    "        h_w_c = patches.shape[-1]\n",
    "        patch_reshape = tf.reshape(patches, [batch_size, -1, h_w_c]) # (batch_size, no_of_patches, h*w*c)\n",
    "        \n",
    "        return patch_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7363dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed_Position(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_patches, projection_dims):\n",
    "        \n",
    "        super(Embed_Position, self).__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.project = tf.keras.layers.Dense(units=projection_dims)\n",
    "        self.embed = tf.keras.layers.Embedding(input_dim=num_patches, output_dim=projection_dims)\n",
    "    \n",
    "    def call(self, patch):\n",
    "        \n",
    "        position = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encode = self.project(patch) + self.embed(position)\n",
    "        \n",
    "        return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea8b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_mixer(inputs, no_units, drop_out):\n",
    "    \n",
    "    for units in no_units:\n",
    "        x = tf.keras.layers.Dense(units=units, activation = tf.nn.gelu)(inputs)\n",
    "        x = tf.keras.layers.Dropout(drop_out)(x)\n",
    "        # x = tf.keras.layers.Dense(inputs.shape[-1], activation=tf.nn.gelu)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d21b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mo_vit_trans_decomp():\n",
    "    \n",
    "    outputs = []\n",
    "    no_of_nodes = len(mu)\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    #x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    returned_patches = Generate_Patches(patch_size)(inputs)\n",
    "    encoded_patches = Embed_Position(no_patches, dims)(returned_patches)\n",
    "    #encoded_patches = tf.keras.layers.BatchNormalization()(encoded_patches)\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(no_transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        input_two_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(num_heads=no_heads, key_dim=dims, dropout=0.1)(input_two_attention, input_two_attention)\n",
    "        # Skip connection 1.\n",
    "        input_two_attention_2 = tf.keras.layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        input_two_attention_3  = tf.keras.layers.LayerNormalization(epsilon=1e-6)(input_two_attention_2)\n",
    "        # MLP.\n",
    "        input_two_attention_3 = mlp_mixer(input_two_attention_3, no_units=transformer_units, drop_out=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = tf.keras.layers.Add()([input_two_attention_3, input_two_attention_3])\n",
    "        \n",
    "        \n",
    "# Create a [batch_size, projection_dim] tensor.\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = tf.keras.layers.Flatten()(representation)\n",
    "    representation = tf.keras.layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp_mixer(representation, no_units=mlp_head_units, drop_out=0.5)\n",
    "    # Classify outputs.\n",
    "    \n",
    "    for i in range(1, no_of_nodes+1):\n",
    "        output = tf.keras.layers.Dense(output_shape, activation='sigmoid', name='output_{}'.format(i))(features)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"sEMG-Decomposition\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbd65d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sEMG-Decomposition\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 120, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " generate__patches (Generate_Pa  (None, None, 144)   0           ['input_1[0][0]']                \n",
      " tches)                                                                                           \n",
      "                                                                                                  \n",
      " embed__position (Embed_Positio  (None, 50, 100)     19500       ['generate__patches[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 50, 100)     200         ['embed__position[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 50, 100)     80700       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 50, 100)      0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'embed__position[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 50, 100)     200         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 100)      10100       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 100)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 50, 100)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 50, 100)     200         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 50, 100)     80700       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 50, 100)      0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 50, 100)     200         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50, 100)      10100       ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 50, 100)      0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 50, 100)      0           ['dropout_3[0][0]',              \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 50, 100)     200         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 50, 100)     80700       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 50, 100)      0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 50, 100)     200         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 50, 100)      10100       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 50, 100)      0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 50, 100)      0           ['dropout_5[0][0]',              \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 50, 100)     200         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 50, 100)     80700       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 50, 100)      0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 50, 100)     200         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 50, 100)      10100       ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_7 (Dropout)            (None, 50, 100)      0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 50, 100)      0           ['dropout_7[0][0]',              \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 50, 100)     200         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 5000)         0           ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 5000)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 10)           50010       ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 10)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " output_1 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_2 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_3 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_4 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_5 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_6 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_7 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_8 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_9 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " output_10 (Dense)              (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 434,620\n",
      "Trainable params: 434,620\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = mo_vit_trans_decomp()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d57b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_callback = AccuracyCallback('accuracy')\n",
    "f1_callback = AccuracyCallback('f1_m')\n",
    "\n",
    "\n",
    "n_batch = 128\n",
    "n_epochs = 3\n",
    "ls =  'binary_crossentropy'\n",
    "mtr = ['mse', 'accuracy',  f1_m]\n",
    "opt = 'rmsprop'\n",
    "model.compile(optimizer=opt, loss=ls, metrics=mtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b31df261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "129/129 [==============================] - 27s 112ms/step - loss: 6.6607 - output_1_loss: 0.6518 - output_2_loss: 0.6699 - output_3_loss: 0.6629 - output_4_loss: 0.6932 - output_5_loss: 0.6615 - output_6_loss: 0.6592 - output_7_loss: 0.6550 - output_8_loss: 0.6954 - output_9_loss: 0.6450 - output_10_loss: 0.6667 - output_1_mse: 0.2288 - output_1_accuracy: 0.9200 - output_1_f1_m: 0.0193 - output_2_mse: 0.2338 - output_2_accuracy: 0.9202 - output_2_f1_m: 0.0174 - output_3_mse: 0.2335 - output_3_accuracy: 0.9237 - output_3_f1_m: 0.0223 - output_4_mse: 0.2439 - output_4_accuracy: 0.9187 - output_4_f1_m: 0.0114 - output_5_mse: 0.2340 - output_5_accuracy: 0.9248 - output_5_f1_m: 0.0163 - output_6_mse: 0.2320 - output_6_accuracy: 0.9192 - output_6_f1_m: 0.0141 - output_7_mse: 0.2306 - output_7_accuracy: 0.9248 - output_7_f1_m: 0.0142 - output_8_mse: 0.2448 - output_8_accuracy: 0.9266 - output_8_f1_m: 0.0040 - output_9_mse: 0.2271 - output_9_accuracy: 0.9248 - output_9_f1_m: 0.0131 - output_10_mse: 0.2351 - output_10_accuracy: 0.9347 - output_10_f1_m: 0.0037 - val_loss: 6.2946 - val_output_1_loss: 0.6293 - val_output_2_loss: 0.6301 - val_output_3_loss: 0.6299 - val_output_4_loss: 0.6308 - val_output_5_loss: 0.6287 - val_output_6_loss: 0.6295 - val_output_7_loss: 0.6295 - val_output_8_loss: 0.6297 - val_output_9_loss: 0.6286 - val_output_10_loss: 0.6286 - val_output_1_mse: 0.2181 - val_output_1_accuracy: 0.9764 - val_output_1_f1_m: 0.0000e+00 - val_output_2_mse: 0.2185 - val_output_2_accuracy: 0.9735 - val_output_2_f1_m: 0.0000e+00 - val_output_3_mse: 0.2184 - val_output_3_accuracy: 0.9774 - val_output_3_f1_m: 0.0000e+00 - val_output_4_mse: 0.2189 - val_output_4_accuracy: 0.9762 - val_output_4_f1_m: 0.0000e+00 - val_output_5_mse: 0.2178 - val_output_5_accuracy: 0.9828 - val_output_5_f1_m: 0.0000e+00 - val_output_6_mse: 0.2182 - val_output_6_accuracy: 0.9772 - val_output_6_f1_m: 0.0000e+00 - val_output_7_mse: 0.2182 - val_output_7_accuracy: 0.9752 - val_output_7_f1_m: 0.0000e+00 - val_output_8_mse: 0.2183 - val_output_8_accuracy: 0.9842 - val_output_8_f1_m: 0.0000e+00 - val_output_9_mse: 0.2178 - val_output_9_accuracy: 0.9796 - val_output_9_f1_m: 0.0000e+00 - val_output_10_mse: 0.2178 - val_output_10_accuracy: 0.9879 - val_output_10_f1_m: 0.0000e+00\n",
      "Epoch 2/3\n",
      "129/129 [==============================] - 11s 88ms/step - loss: 6.0449 - output_1_loss: 0.6046 - output_2_loss: 0.6064 - output_3_loss: 0.6059 - output_4_loss: 0.6075 - output_5_loss: 0.6018 - output_6_loss: 0.6035 - output_7_loss: 0.6038 - output_8_loss: 0.6070 - output_9_loss: 0.6006 - output_10_loss: 0.6037 - output_1_mse: 0.2055 - output_1_accuracy: 0.9679 - output_1_f1_m: 0.0000e+00 - output_2_mse: 0.2060 - output_2_accuracy: 0.9676 - output_2_f1_m: 0.0030 - output_3_mse: 0.2059 - output_3_accuracy: 0.9692 - output_3_f1_m: 0.0000e+00 - output_4_mse: 0.2062 - output_4_accuracy: 0.9711 - output_4_f1_m: 0.0000e+00 - output_5_mse: 0.2044 - output_5_accuracy: 0.9784 - output_5_f1_m: 0.0000e+00 - output_6_mse: 0.2052 - output_6_accuracy: 0.9701 - output_6_f1_m: 0.0000e+00 - output_7_mse: 0.2054 - output_7_accuracy: 0.9674 - output_7_f1_m: 0.0071 - output_8_mse: 0.2053 - output_8_accuracy: 0.9798 - output_8_f1_m: 0.0000e+00 - output_9_mse: 0.2039 - output_9_accuracy: 0.9781 - output_9_f1_m: 0.0000e+00 - output_10_mse: 0.2046 - output_10_accuracy: 0.9854 - output_10_f1_m: 0.0010 - val_loss: 5.7448 - val_output_1_loss: 0.5746 - val_output_2_loss: 0.5759 - val_output_3_loss: 0.5750 - val_output_4_loss: 0.5762 - val_output_5_loss: 0.5732 - val_output_6_loss: 0.5748 - val_output_7_loss: 0.5750 - val_output_8_loss: 0.5739 - val_output_9_loss: 0.5736 - val_output_10_loss: 0.5725 - val_output_1_mse: 0.1911 - val_output_1_accuracy: 0.9764 - val_output_1_f1_m: 0.0000e+00 - val_output_2_mse: 0.1917 - val_output_2_accuracy: 0.9735 - val_output_2_f1_m: 0.0000e+00 - val_output_3_mse: 0.1913 - val_output_3_accuracy: 0.9774 - val_output_3_f1_m: 0.0000e+00 - val_output_4_mse: 0.1919 - val_output_4_accuracy: 0.9762 - val_output_4_f1_m: 0.0000e+00 - val_output_5_mse: 0.1904 - val_output_5_accuracy: 0.9828 - val_output_5_f1_m: 0.0000e+00 - val_output_6_mse: 0.1912 - val_output_6_accuracy: 0.9772 - val_output_6_f1_m: 0.0000e+00 - val_output_7_mse: 0.1913 - val_output_7_accuracy: 0.9752 - val_output_7_f1_m: 0.0000e+00 - val_output_8_mse: 0.1907 - val_output_8_accuracy: 0.9842 - val_output_8_f1_m: 0.0000e+00 - val_output_9_mse: 0.1906 - val_output_9_accuracy: 0.9796 - val_output_9_f1_m: 0.0000e+00 - val_output_10_mse: 0.1900 - val_output_10_accuracy: 0.9879 - val_output_10_f1_m: 0.0000e+00\n",
      "Epoch 3/3\n",
      "129/129 [==============================] - 12s 95ms/step - loss: 5.5949 - output_1_loss: 0.5525 - output_2_loss: 0.5558 - output_3_loss: 0.5712 - output_4_loss: 0.5672 - output_5_loss: 0.5474 - output_6_loss: 0.5635 - output_7_loss: 0.5649 - output_8_loss: 0.5624 - output_9_loss: 0.5468 - output_10_loss: 0.5631 - output_1_mse: 0.1801 - output_1_accuracy: 0.9676 - output_1_f1_m: 0.0000e+00 - output_2_mse: 0.1813 - output_2_accuracy: 0.9663 - output_2_f1_m: 0.0014 - output_3_mse: 0.1831 - output_3_accuracy: 0.9664 - output_3_f1_m: 4.8450e-04 - output_4_mse: 0.1828 - output_4_accuracy: 0.9692 - output_4_f1_m: 4.8450e-04 - output_5_mse: 0.1779 - output_5_accuracy: 0.9792 - output_5_f1_m: 0.0000e+00 - output_6_mse: 0.1823 - output_6_accuracy: 0.9679 - output_6_f1_m: 9.3963e-04 - output_7_mse: 0.1824 - output_7_accuracy: 0.9659 - output_7_f1_m: 4.9219e-04 - output_8_mse: 0.1810 - output_8_accuracy: 0.9781 - output_8_f1_m: 4.8450e-04 - output_9_mse: 0.1776 - output_9_accuracy: 0.9784 - output_9_f1_m: 0.0000e+00 - output_10_mse: 0.1805 - output_10_accuracy: 0.9825 - output_10_f1_m: 0.0000e+00 - val_loss: 5.2362 - val_output_1_loss: 0.5242 - val_output_2_loss: 0.5256 - val_output_3_loss: 0.5245 - val_output_4_loss: 0.5256 - val_output_5_loss: 0.5219 - val_output_6_loss: 0.5242 - val_output_7_loss: 0.5247 - val_output_8_loss: 0.5224 - val_output_9_loss: 0.5227 - val_output_10_loss: 0.5205 - val_output_1_mse: 0.1665 - val_output_1_accuracy: 0.9764 - val_output_1_f1_m: 0.0000e+00 - val_output_2_mse: 0.1673 - val_output_2_accuracy: 0.9735 - val_output_2_f1_m: 0.0000e+00 - val_output_3_mse: 0.1667 - val_output_3_accuracy: 0.9774 - val_output_3_f1_m: 0.0000e+00 - val_output_4_mse: 0.1672 - val_output_4_accuracy: 0.9762 - val_output_4_f1_m: 0.0000e+00 - val_output_5_mse: 0.1654 - val_output_5_accuracy: 0.9828 - val_output_5_f1_m: 0.0000e+00 - val_output_6_mse: 0.1665 - val_output_6_accuracy: 0.9772 - val_output_6_f1_m: 0.0000e+00 - val_output_7_mse: 0.1668 - val_output_7_accuracy: 0.9752 - val_output_7_f1_m: 0.0000e+00 - val_output_8_mse: 0.1656 - val_output_8_accuracy: 0.9842 - val_output_8_f1_m: 0.0000e+00 - val_output_9_mse: 0.1658 - val_output_9_accuracy: 0.9796 - val_output_9_f1_m: 0.0000e+00 - val_output_10_mse: 0.1647 - val_output_10_accuracy: 0.9879 - val_output_10_f1_m: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x,y, validation_split=0.2, batch_size=n_batch, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "600cd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Validation\n",
    "\n",
    "x_ts, y_ts = load_test(path, test_ls, overlap,mu)\n",
    "x_ts = np.expand_dims(x_ts, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b7190a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - 13s 40ms/step - loss: 5.2444 - output_1_loss: 0.5260 - output_2_loss: 0.5269 - output_3_loss: 0.5260 - output_4_loss: 0.5265 - output_5_loss: 0.5229 - output_6_loss: 0.5257 - output_7_loss: 0.5263 - output_8_loss: 0.5224 - output_9_loss: 0.5227 - output_10_loss: 0.5189 - output_1_mse: 0.1674 - output_1_accuracy: 0.9718 - output_1_f1_m: 0.0000e+00 - output_2_mse: 0.1679 - output_2_accuracy: 0.9703 - output_2_f1_m: 0.0000e+00 - output_3_mse: 0.1675 - output_3_accuracy: 0.9734 - output_3_f1_m: 0.0000e+00 - output_4_mse: 0.1677 - output_4_accuracy: 0.9739 - output_4_f1_m: 0.0000e+00 - output_5_mse: 0.1659 - output_5_accuracy: 0.9804 - output_5_f1_m: 0.0000e+00 - output_6_mse: 0.1673 - output_6_accuracy: 0.9733 - output_6_f1_m: 0.0000e+00 - output_7_mse: 0.1676 - output_7_accuracy: 0.9712 - output_7_f1_m: 0.0000e+00 - output_8_mse: 0.1657 - output_8_accuracy: 0.9840 - output_8_f1_m: 0.0000e+00 - output_9_mse: 0.1658 - output_9_accuracy: 0.9794 - output_9_f1_m: 0.0000e+00 - output_10_mse: 0.1639 - output_10_accuracy: 0.9919 - output_10_f1_m: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.244436740875244,\n",
       " 0.5259920954704285,\n",
       " 0.5269360542297363,\n",
       " 0.5260438323020935,\n",
       " 0.5265290141105652,\n",
       " 0.5228788256645203,\n",
       " 0.5257205963134766,\n",
       " 0.5262925028800964,\n",
       " 0.5224440693855286,\n",
       " 0.5227491855621338,\n",
       " 0.5188524127006531,\n",
       " 0.1674339771270752,\n",
       " 0.9718213081359863,\n",
       " 0.0,\n",
       " 0.16789530217647552,\n",
       " 0.9702503681182861,\n",
       " 0.0,\n",
       " 0.16745100915431976,\n",
       " 0.9733922481536865,\n",
       " 0.0,\n",
       " 0.16768145561218262,\n",
       " 0.9738831520080566,\n",
       " 0.0,\n",
       " 0.165893092751503,\n",
       " 0.9803632497787476,\n",
       " 0.0,\n",
       " 0.1672961264848709,\n",
       " 0.9732940793037415,\n",
       " 0.0,\n",
       " 0.16758021712303162,\n",
       " 0.9712321758270264,\n",
       " 0.0,\n",
       " 0.1656656116247177,\n",
       " 0.9839960932731628,\n",
       " 0.0,\n",
       " 0.16583454608917236,\n",
       " 0.9793814420700073,\n",
       " 0.0,\n",
       " 0.1638983190059662,\n",
       " 0.9919489622116089,\n",
       " 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_ts, y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "083136d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_motor = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "279bcf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319/319 [==============================] - 5s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_ts)\n",
    "y_pred = np.argmax(pred[select_motor], axis=-1)\n",
    "confusion = confusion_matrix(y_ts[select_motor], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ebb7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[0, 0]\n",
    "TN = confusion[1, 1]\n",
    "FP = confusion[1, 0]\n",
    "FN = confusion[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "963e67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.9919489445262641\n"
     ]
    }
   ],
   "source": [
    "precision = TP / float(TP + FP)\n",
    "print(\"Precision is {}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7036f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall is 1.0\n"
     ]
    }
   ],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "print(\"recall is {}\".format(recall))\n",
    "\n",
    "#RECALL SAME AS SENSITIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34165b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: decompose\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: decompose\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('decompose')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decomp",
   "language": "python",
   "name": "decomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
