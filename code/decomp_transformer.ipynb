{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb843432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\decomp\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from decomp_utils import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6c262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\AI-Workspace\\\\Decomposition\\\\Yue\\\\data\\\\5_50_GM-\"\n",
    "train_ls = [1, 3]\n",
    "test_ls = 2\n",
    "overlap = 5\n",
    "x_sEMG = 'EMGs'\n",
    "y_spikes = 'Spikes'\n",
    "mu = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d232506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_train(path, train_ls, overlap,mu)\n",
    "x = np.expand_dims(x, axis=3)   #2D Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877ece78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x.shape[1:]\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c1d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameter\n",
    "\n",
    "input_shape = input_shape\n",
    "patch_size = 8\n",
    "no_patches = 120\n",
    "dims = 100\n",
    "no_transformer_layers = 4\n",
    "no_heads = 2\n",
    "transformer_units = [dims * 2,dims]  # Size of the transformer layers\n",
    "mlp_head_units = [240, 10]   # [2048, 1024]\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3b2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_Patches(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, patch_size):\n",
    "        super(Generate_Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    \n",
    "    def call(self, images):\n",
    "        \n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(images=images,\n",
    "                           sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "                           strides=[1, self.patch_size, self.patch_size, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')\n",
    "        h_w_c = patches.shape[-1]\n",
    "        patch_reshape = tf.reshape(patches, [batch_size, -1, h_w_c]) # (batch_size, no_of_patches, h*w*c)\n",
    "        \n",
    "        return patch_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0c7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed_Position(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_patches, projection_dims):\n",
    "        \n",
    "        super(Embed_Position, self).__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.project = tf.keras.layers.Dense(units=projection_dims)\n",
    "        self.embed = tf.keras.layers.Embedding(input_dim=num_patches, output_dim=projection_dims)\n",
    "    \n",
    "    def call(self, patch):\n",
    "        \n",
    "        position = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encode = self.project(patch) + self.embed(position)\n",
    "        \n",
    "        return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b5e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_mixer(inputs, no_units, drop_out):\n",
    "    \n",
    "    for units in no_units:\n",
    "        x = tf.keras.layers.Dense(units=units, activation = tf.nn.gelu)(inputs)\n",
    "        x = tf.keras.layers.Dropout(drop_out)(x)\n",
    "        # x = tf.keras.layers.Dense(inputs.shape[-1], activation=tf.nn.gelu)(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120d45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_trans_decomp():\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    #x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    returned_patches = Generate_Patches(patch_size)(inputs)\n",
    "    encoded_patches = Embed_Position(no_patches, dims)(returned_patches)\n",
    "    #encoded_patches = tf.keras.layers.BatchNormalization()(encoded_patches)\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(no_transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        input_two_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(num_heads=no_heads, key_dim=dims, dropout=0.1)(input_two_attention, input_two_attention)\n",
    "        # Skip connection 1.\n",
    "        input_two_attention_2 = tf.keras.layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        input_two_attention_3  = tf.keras.layers.LayerNormalization(epsilon=1e-6)(input_two_attention_2)\n",
    "        # MLP.\n",
    "        input_two_attention_3 = mlp_mixer(input_two_attention_3, no_units=transformer_units, drop_out=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = tf.keras.layers.Add()([input_two_attention_3, input_two_attention_3])\n",
    "        \n",
    "        \n",
    "# Create a [batch_size, projection_dim] tensor.\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = tf.keras.layers.Flatten()(representation)\n",
    "    representation = tf.keras.layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp_mixer(representation, no_units=mlp_head_units, drop_out=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = tf.keras.layers.Dense(1, activation='sigmoid')(features)\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits, name=\"sEMG-Decomposition\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df8ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sEMG-Decomposition\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 120, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " generate__patches (Generate_Pa  (None, None, 64)    0           ['input_1[0][0]']                \n",
      " tches)                                                                                           \n",
      "                                                                                                  \n",
      " embed__position (Embed_Positio  (None, 120, 100)    18500       ['generate__patches[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 120, 100)    200         ['embed__position[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 120, 100)    80700       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 120, 100)     0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'embed__position[0][0]']        \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 120, 100)    200         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 120, 100)     10100       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 120, 100)     0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 120, 100)     0           ['dropout_1[0][0]',              \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 120, 100)    200         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 120, 100)    80700       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 120, 100)     0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 120, 100)    200         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 120, 100)     10100       ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 120, 100)     0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 120, 100)     0           ['dropout_3[0][0]',              \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 120, 100)    200         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 120, 100)    80700       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 120, 100)     0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 120, 100)    200         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 120, 100)     10100       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 120, 100)     0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 120, 100)     0           ['dropout_5[0][0]',              \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 120, 100)    200         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 120, 100)    80700       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 120, 100)     0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 120, 100)    200         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 120, 100)     10100       ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_7 (Dropout)            (None, 120, 100)     0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 120, 100)     0           ['dropout_7[0][0]',              \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 120, 100)    200         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 12000)        0           ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 12000)        0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 10)           120010      ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 10)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            11          ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 503,521\n",
      "Trainable params: 503,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = vit_trans_decomp()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc0fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_callback = AccuracyCallback('accuracy')\n",
    "f1_callback = AccuracyCallback('f1_m')\n",
    "\n",
    "\n",
    "n_batch = 8\n",
    "n_epochs = 10\n",
    "ls =  'binary_crossentropy'\n",
    "mtr = ['mse', 'accuracy',  f1_m]\n",
    "opt = 'rmsprop'\n",
    "model.compile(optimizer=opt, loss=ls, metrics=mtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc683179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2059/2059 [==============================] - 90s 40ms/step - loss: 0.2453 - mse: 0.0452 - accuracy: 0.9651 - f1_m: 0.0000e+00 - val_loss: 0.1054 - val_mse: 0.0227 - val_accuracy: 0.9764 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2059/2059 [==============================] - 84s 41ms/step - loss: 0.1825 - mse: 0.0348 - accuracy: 0.9685 - f1_m: 0.0000e+00 - val_loss: 0.1156 - val_mse: 0.0231 - val_accuracy: 0.9764 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1218/2059 [================>.............] - ETA: 31s - loss: 0.1708 - mse: 0.0331 - accuracy: 0.9683 - f1_m: 4.1051e-04"
     ]
    }
   ],
   "source": [
    "history = model.fit(x,y, validation_split=0.2, batch_size=n_batch, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Validation\n",
    "\n",
    "x_ts, y_ts = load_test(path, test_ls, overlap,mu)\n",
    "x_ts = np.expand_dims(x_ts, axis=3)   #2D Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_ts, y_ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decomp",
   "language": "python",
   "name": "decomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
